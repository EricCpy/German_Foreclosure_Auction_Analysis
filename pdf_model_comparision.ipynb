{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\", filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\") \n",
    "# # LLama 3.2 sehr schlechtes Ergebnis aber nur 20sec zum Generieren\n",
    "# downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\", filename=\"Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf\") \n",
    "# # LLama 3.1 sehr gutes Ergebnis, sehr dauert auch nur 20sec \n",
    "#downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Mistral-Nemo-Instruct-2407-GGUF\", filename=\"Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\") \n",
    "# 6_K_L: Mistral Nemo sehr gutes Ergebnis extrem langsam 3min, eventuell könnte 3 gut sein oder 4, Q4_K_M: sehr gutes Ergebnis, dauert ungefähr 1min\n",
    "# downloaded_model_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\", filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\") \n",
    "# normal Mistral schlechtes Ergebnis, aber besser als llama 3.2 dauert so lange wie llama 3.2\n",
    "downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Mistral-NeMo-Minitron-8B-Instruct-GGUF\", filename=\"Mistral-NeMo-Minitron-8B-Instruct-Q4_K_L.gguf\") \n",
    "# Q6_K_L Minitron sehr gutes Ergebnis, dauert eine Minute, Q5_K_L und Q4_K_L jeweils pro Q1 10sec schneller (Q5_K_M, Q4_K_M): buggy liefert nicht den richtigen Output\n",
    "\n",
    "# Modelle, die ich miteinander vergleichen will:\n",
    "# Minitron\n",
    "# LLama 3.1\n",
    "# LLama 3.2\n",
    "# -> bestes Model, will ich finetunen und gegen das normale Model als Baseline vergleichen\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Heizsystem(str, Enum):\n",
    "    ZENTRAL = \"Zentralheizung\"\n",
    "    ETAGEN = \"Etagenheizung\"\n",
    "    FERN = \"Fernwärme\"\n",
    "    ELEKTRISCH = \"Elektrische Heizung\"\n",
    "    OFEN = \"Ofenheizung\"\n",
    "\n",
    "class RaumTyp(str, Enum):\n",
    "    WOHNZIMMER = \"Wohnzimmer\"\n",
    "    SCHLAFZIMMER = \"Schlafzimmer\"\n",
    "    KUECHE = \"Küche\"\n",
    "    BADEZIMMER = \"Badezimmer\"\n",
    "    ARBEITSZIMMER = \"Arbeitszimmer\"\n",
    "    HAUSWIRTSCHAFTSRAUM = \"Hauswirtschaftsraum\"\n",
    "    FLUR = \"Flur\"\n",
    "    ABSTELLRAUM = \"Abstellraum\"\n",
    "    ESSZIMMER = \"Esszimmer\"\n",
    "\n",
    "class ForclosureObject(BaseModel):\n",
    "    flaeche: int = Field(description=\"Die Fläche des Objekts in Quadratmetern.\")\n",
    "    #beschreibung: str = Field(description=\"Beschreibung des Zwangsversteigerungsobjekts.\")\n",
    "    vermietet: bool = Field(description=\"Gibt an, ob das Objekt vermietet ist.\")\n",
    "    verkehrswert: int = Field(description=\"Verkehrswert des Objekts.\")\n",
    "    miete: Optional[str] = Field(description=\"Monatliche Mieteinnahmen für das Objekt, falls vermietet.\")\n",
    "    typ: Optional[str] = Field(description=\"Art der Immobilie (z.B. Wohnung, Haus).\")\n",
    "    heizsystem: Optional[Heizsystem] = Field(description=\"Art des Heizsystems.\")\n",
    "    baujahr: Optional[str] = Field(description=\"Baujahr der Immobilie.\")\n",
    "    raeume: Optional[int] = Field(description=\"Anzahl der Räume im Objekt.\")\n",
    "    raum_typen: List[RaumTyp] = Field(default_factory=list, description=\"\"\"Liste der Raumtypen im Objekt (z.B. Wohnzimmer, Küche).\n",
    "                                      Die Anzahl der Einträge in der Liste sollte der Gesamtzahl der Räume entsprechen. \n",
    "                                      Mehrere Räume desselben Typs sollten jeweils einzeln aufgeführt werden.\"\"\")\n",
    "\n",
    "class Forclosure(BaseModel):\n",
    "    objekte: List[ForclosureObject] = Field(default_factory=list, description=\"Liste der Zwangsversteigerungsobjekte, die zu diesem Fall gehören.\")\n",
    "    gesamtverkehrswert: int = Field(description=\"Gesamtverkehrswert aller Zwangsversteigerungsobjekte.\")\n",
    "    #beschreibung: str = Field(description=\"Allgemeine Beschreibung des Zwangsversteigerungsfalls.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=downloaded_model_path, n_ctx=4096, n_threads=8, n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def pdf_to_string(file_path):\n",
    "    pdf_document = pymupdf.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "file_path = \"60782.pdf\"\n",
    "pdf_text = pdf_to_string(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from llama_cpp import LogitsProcessorList\n",
    "from lmformatenforcer import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))\n",
    "\n",
    "def llamacpp_with_character_level_parser(llm: Llama, prompt: str, character_level_parser: Optional[CharacterLevelParser]) -> str:\n",
    "    logits_processors: Optional[LogitsProcessorList] = None\n",
    "    if character_level_parser:\n",
    "        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm, character_level_parser)])\n",
    "    \n",
    "    output = llm(prompt, logits_processor=logits_processors, max_tokens=1000)\n",
    "    text: str = output['choices'][0]['text']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_with_schema = f'Please extract information about {pdf_text}. You MUST answer using the following json schema: {Forclosure.model_json_schema()}'\n",
    "prompt = get_prompt(question_with_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**LLM Output with json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "      \n",
       "{\n",
       "  \"objekte\": [\n",
       "    {\n",
       "      \"flaeche\": 67,\n",
       "      \"vermietet\": false,\n",
       "      \"verkehrswert\": 250000,\n",
       "      \"miete\":  \"410,36\",\n",
       "      \"typ\": \"Wohnung\",\n",
       "      \"heizsystem\": \"Zentralheizung\",\n",
       "      \"baujahr\": \"ca. 1900\",\n",
       "      \"raeume\": 5,\n",
       "      \"raum_typen\": [\n",
       "        \"Wohnzimmer\",\n",
       "        \"Küche\",\n",
       "        \"Badezimmer\",\n",
       "        \"Hauswirtschaftsraum\",\n",
       "        \"Flur\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"flaeche\": 64,\n",
       "      \"vermietet\": false,\n",
       "      \"verkehrswert\": 240000,\n",
       "      \"miete\":  \"202,24\",\n",
       "      \"typ\": \"Wohnung\",\n",
       "      \"heizsystem\": \"Zentralheizung\",\n",
       "      \"baujahr\": \"ca. 1900\",\n",
       "      \"raeume\": 4,\n",
       "      \"raum_typen\": [\n",
       "        \"Schlafzimmer\",\n",
       "        \"Schlafzimmer\",\n",
       "        \"Badezimmer\",\n",
       "        \"Flur\"\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"gesamtverkehrswert\": 580000\n",
       "}\n",
       " \n",
       "  \n",
       " \n",
       " \n",
       " \n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_header(\"LLM Output with json schema enforcing:\")\n",
    "result = llamacpp_with_character_level_parser(llm, prompt, JsonSchemaParser(Forclosure.model_json_schema()))\n",
    "display_content(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
