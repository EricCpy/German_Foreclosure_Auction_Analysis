{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Scraping Offical German Government Website\n",
    "- germany has different federal states and each federal state has kreise and each kreis has an amtsgericht in which the Zwangsversteigerungen take place\n",
    "- each amtsgericht uploads the zwangsversteigerungen to https://www.zvg-portal.de/index.php?button=Suchen&all=1, this offers only a generale overview\n",
    "- there are different of information in our data, the general description, the exposee, gutachten and amtliche bekanntmachung and fotos\n",
    "- we only want to focus on the general description, amtliche bekanntmachung and exposee when scraping, we will safe the gutachten as external link and the fotos, most of the time the fotos arent actually really helpful, because its only pictures from outside the apartment or house and the grundrisse are also meh, but we will safe this as external links, if we want to use this data later\n",
    "- the gutachten has also some interesting informations, but from the gutachten we get an information overload, what we could try is to feat \n",
    "- value they delivier and test especially for Berlin if we get better predictions in our model with these informations (looking into regression by picture or translating a picture into text) \n",
    "- then there are are websites which seem to scrape the government website and display it with less information but in a more stylized way like (https://versteigerungspool.de/, https://www.zwangsversteigerung.de/ or https://www.zvg-online.net/), we dont want to scrape such sites, because they offer us no additional value, except for easier scraping in some parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- https://www.zvg-portal.de/ (official website of the german government where foreclosure auctions can be found for selected regional courts) \n",
    "  - (Effort: uniform general description, more detailed information in unstructured PDFs, which would need to be extracted)\n",
    "  - https://www.zvg-portal.de/index.php?button=Suchen&all=1\n",
    "  - alle 16 Bundesländer haben einen kürzel auf dieser Website, diese Kürzel kann ich entweder aus https://www.zvg-portal.de/index.php?button=Termine%20suchen entnehmen oder als constante speichern, bevorzugter Weise von der Webseite entnehmen\n",
    "  - Alerting, wenn für ein Bundesland keine neuen Daten kamen\n",
    "  - pandas df in sqllite db speichern anstatt in csv\n",
    "  - PDF Daten müssen vorher schon entweder als string mit in die DB gegeben werden oder am besten schon gleich processen, damit ich nicht zu viele Daten speichere\n",
    "  - Gutachten, Amtliche Bekanntmachung, Exposee erstmal als external Links speichern\n",
    "  - How to scrape:\n",
    "    - go to https://www.zvg-portal.de/index.php?button=Termine%20suchen form -> iterate through table first tr td should contain Land\n",
    "    - if it contains Land then do: go to next tr.td.select inside select are options take each option value attribute except for 0\n",
    "    - after having all values do a post request for each saved value https://www.zvg-portal.de/index.php?button=Suchen&all=1 with body ger_name=--+Alle+Amtsgerichte+--&order_by=2&land_abk=value&ger_id=0&az1=&az2=&az3=&az4=&art=&obj=&str=&hnr=&plz=&ort=&ortsteil=&vtermin=&btermin=\n",
    "      - inside form the second table go through all trs if tr contains three tds, go to the second td, get the a element link \n",
    "      - open the link\n",
    "      - get table first tr contains in first td aktenzeichen and in second td letzte Aktualisierung\n",
    "      - after first tr go through all other trs if first td in a tr equals to key[: -1] in keywords (keywords={\"Art der Versteigerung\", \"Grundbuch\", \"Objekt/Lage\", \"Beschreibung\", \"Verkehrswert in €\", \"Termin\", \"Ort der Versteigerung\", \"amtliche Bekanntmachung\", \"Exposee\", \"Gutachten\"}) then safe the value if value contains a then safe the link\n",
    "      - if key == \"Foto\" then safe link in list\n",
    "      - after finishing iteration safe foto list in fotos and append everything saved as new row to an sqlite db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords dictionary for extracting specific table rows\n",
    "keywords = {\n",
    "    \"Art der Versteigerung\", \"Grundbuch\", \"Objekt/Lage\", \"Beschreibung\", \n",
    "    \"Verkehrswert in €\", \"Termin\", \"Ort der Versteigerung\", \n",
    "    \"amtliche Bekanntmachung\", \"Exposee\", \"Gutachten\"\n",
    "}\n",
    "\n",
    "# Create an SQLite connection and a table\n",
    "conn = sqlite3.connect('foreclosures.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS foreclosure_data (\n",
    "        aktenzeichen TEXT,\n",
    "        letzte_aktualisierung TEXT,\n",
    "        art_der_versteigerung TEXT,\n",
    "        grundbuch TEXT,\n",
    "        objekt_lage TEXT,\n",
    "        beschreibung TEXT,\n",
    "        verkehrswert TEXT,\n",
    "        termin TEXT,\n",
    "        ort_der_versteigerung TEXT,\n",
    "        amtliche_bekanntmachung TEXT,\n",
    "        exposee TEXT,\n",
    "        gutachten TEXT,\n",
    "        fotos TEXT\n",
    "    )\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Federal state codes\n",
    "- TODO: Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.zvg-portal.de/index.php?button=Termine%20suchen\"\n",
    "response = requests.get(url)\n",
    "search_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "land_select = None\n",
    "for tr in search_soup.find_all('tr'):\n",
    "    if \"Land\" in tr.text:\n",
    "        land_select = tr.find_next('select')\n",
    "        break\n",
    "\n",
    "if not land_select:\n",
    "    print(\"Land select field not found.\")\n",
    "    exit()\n",
    "\n",
    "land_codes = [option['value'] for option in land_select.find_all('option') if option['value'] != \"0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auctions\n",
    "- TODO: Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for land_code in land_codes:\n",
    "    post_url = \"https://www.zvg-portal.de/index.php?button=Suchen&all=1\"\n",
    "    post_data = {\n",
    "        'ger_name': '--+Alle+Amtsgerichte+--',\n",
    "        'order_by': '2',\n",
    "        'land_abk': land_code,\n",
    "        'ger_id': '0'\n",
    "    }\n",
    "    \n",
    "    post_response = requests.post(post_url, data=post_data)\n",
    "    if not str(post_response.status_code).startswith(\"2\"):\n",
    "        print(f\"Request for {land_code} failed!\")\n",
    "        continue\n",
    "    \n",
    "    land_soup = BeautifulSoup(post_response.text, 'html.parser')\n",
    "    # TODO folgendes in eine Funktion auslagern\n",
    "    \n",
    "    # Iterate over the second table and find the link for each foreclosure case\n",
    "    result_table = land_soup.find_all('table')[1]  # Second table\n",
    "    for tr in result_table.find_all('tr'):\n",
    "        tds = tr.find_all('td')\n",
    "        if len(tds) == 3:\n",
    "            link = tds[1].find('a')['href']\n",
    "            case_url = f\"https://www.zvg-portal.de/{link}\"\n",
    "            \n",
    "            # Open the case link and extract details\n",
    "            case_response = requests.get(case_url)\n",
    "            case_soup = BeautifulSoup(case_response.text, 'html.parser')\n",
    "            \n",
    "            # Find the first table and extract Aktenzeichen and Letzte Aktualisierung\n",
    "            details_table = case_soup.find_all('table')[0]\n",
    "            rows = details_table.find_all('tr')\n",
    "            aktenzeichen = rows[0].find_all('td')[1].text.strip()\n",
    "            letzte_aktualisierung = rows[1].find_all('td')[1].text.strip()\n",
    "            \n",
    "            # Prepare a dictionary to store the extracted data\n",
    "            case_data = {\n",
    "                'aktenzeichen': aktenzeichen,\n",
    "                'letzte_aktualisierung': letzte_aktualisierung,\n",
    "                'art_der_versteigerung': None,\n",
    "                'grundbuch': None,\n",
    "                'objekt_lage': None,\n",
    "                'beschreibung': None,\n",
    "                'verkehrswert': None,\n",
    "                'termin': None,\n",
    "                'ort_der_versteigerung': None,\n",
    "                'amtliche_bekanntmachung': None,\n",
    "                'exposee': None,\n",
    "                'gutachten': None,\n",
    "                'fotos': []\n",
    "            }\n",
    "            \n",
    "            # Iterate through the remaining rows to extract keyword-related data\n",
    "            for row in rows[1:]:\n",
    "                first_td = row.find_all('td')[0].text.strip()[:-1]\n",
    "                if first_td in keywords:\n",
    "                    value = row.find_all('td')[1].text.strip()\n",
    "                    case_data[first_td] = value\n",
    "                    if 'href' in str(row):\n",
    "                        link = row.find_all('a')[0]['href']\n",
    "                        case_data[first_td] = link\n",
    "                    if first_td == \"Foto\":\n",
    "                        case_data['fotos'].append(link)\n",
    "            \n",
    "            # Save the data in the SQLite database\n",
    "            cur.execute('''\n",
    "                INSERT INTO foreclosure_data \n",
    "                (aktenzeichen, letzte_aktualisierung, art_der_versteigerung, grundbuch, objekt_lage, beschreibung, \n",
    "                verkehrswert, termin, ort_der_versteigerung, amtliche_bekanntmachung, exposee, gutachten, fotos)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                case_data['aktenzeichen'], case_data['letzte_aktualisierung'], case_data['art_der_versteigerung'], \n",
    "                case_data['grundbuch'], case_data['objekt_lage'], case_data['beschreibung'], case_data['verkehrswert'], \n",
    "                case_data['termin'], case_data['ort_der_versteigerung'], case_data['amtliche_bekanntmachung'], \n",
    "                case_data['exposee'], case_data['gutachten'], ','.join(case_data['fotos'])\n",
    "            ))\n",
    "            conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
