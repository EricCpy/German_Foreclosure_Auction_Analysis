{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\", filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\") \n",
    "# # LLama 3.2 sehr schlechtes Ergebnis aber nur 20sec zum Generieren\n",
    "downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\", filename=\"Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf\") \n",
    "# # LLama 3.1 sehr gutes Ergebnis, sehr dauert auch nur 20sec \n",
    "#downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Mistral-Nemo-Instruct-2407-GGUF\", filename=\"Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\") \n",
    "# 6_K_L: Mistral Nemo sehr gutes Ergebnis extrem langsam 3min, eventuell könnte 3 gut sein oder 4, Q4_K_M: sehr gutes Ergebnis, dauert ungefähr 1min\n",
    "# downloaded_model_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\", filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\") \n",
    "# normal Mistral schlechtes Ergebnis, aber besser als llama 3.2 dauert so lange wie llama 3.2\n",
    "#downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Mistral-NeMo-Minitron-8B-Instruct-GGUF\", filename=\"Mistral-NeMo-Minitron-8B-Instruct-Q4_K_L.gguf\") \n",
    "# Q6_K_L Minitron sehr gutes Ergebnis, dauert eine Minute, Q5_K_L und Q4_K_L jeweils pro Q1 10sec schneller (Q5_K_M, Q4_K_M): buggy liefert nicht den richtigen Output\n",
    "# Gemma -> not able to generate json output\n",
    "#downloaded_model_path = hf_hub_download(repo_id=\"bartowski/gemma-2-9b-it-GGUF\", filename=\"gemma-2-9b-it-Q4_K_L.gguf\") \n",
    "\n",
    "# Modelle, die ich miteinander vergleichen will:\n",
    "# LLama 3.1\n",
    "# -> bestes Model, will ich finetunen und gegen das normale Model als Baseline vergleichen\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Zustand(str, Enum):\n",
    "    RENOVIERT = \"Renoviert\"\n",
    "    NEU = \"Neu\"\n",
    "    GEPFLEGT = \"Gepflegt\"\n",
    "    \n",
    "class Heizsystem(str, Enum):\n",
    "    GAS = \"Gas\"\n",
    "    STROM = \"Strom\"\n",
    "    FERN = \"Fernwärme\"\n",
    "    ÖL = \"Öl\"\n",
    "    FESTBRENNSTOFFE = \"Festbrennstoffe\"\n",
    "\n",
    "class RaumTyp(str, Enum):\n",
    "    WOHNZIMMER = \"Wohnzimmer\"\n",
    "    SCHLAFZIMMER = \"Schlafzimmer\"\n",
    "    KUECHE = \"Küche\"\n",
    "    BADEZIMMER = \"Badezimmer\"\n",
    "    ARBEITSZIMMER = \"Arbeitszimmer\"\n",
    "    HAUSWIRTSCHAFTSRAUM = \"Hauswirtschaftsraum\"\n",
    "    FLUR = \"Flur\"\n",
    "    ABSTELLRAUM = \"Abstellraum\"\n",
    "    ESSZIMMER = \"Esszimmer\"\n",
    "\n",
    "class VersteigerungsTyp(str, Enum):\n",
    "    WOHNUNG = \"Wohnung\"\n",
    "    HAUS = \"Haus\"\n",
    "    ANDERES = \"Anderes\"\n",
    "\n",
    "class ForclosureObject(BaseModel):\n",
    "    flaeche: int = Field(description=\"Die Fläche des Objekts in Quadratmetern.\")\n",
    "    # Generiert zu viele \n",
    "    # beschreibung: str = Field(description=\"Beschreibung des Zwangsversteigerungsobjekts.\")\n",
    "    verkehrswert: int = Field(description=\"Verkehrswert des Objekts.\")\n",
    "    typ: Optional[VersteigerungsTyp] = Field(description=\"Art der Immobilie (z.B. Wohnung, Haus oder etwas anderes).\")\n",
    "    baujahr: Optional[int] = Field(description=\"Baujahr der Immobilie.\")\n",
    "    heizsystem: Optional[Heizsystem] = Field(description=\"Art des Heizsystems.\")\n",
    "    # NOTE: Der Zustand wird nicht beschrieben in den Bekanntmachungen, aber in den Exposees, diese sind aber zu lang\n",
    "    # zustand: Optional[Zustand] = Field(description=\"Zustand des Objekts.\")\n",
    "    raeume: Optional[int] = Field(description=\"Anzahl der Räume im Objekt.\")\n",
    "    raum_typen: List[RaumTyp] = Field(default_factory=list, description=\"\"\"Liste der Raumtypen im Objekt (z.B. Wohnzimmer, Küche).\n",
    "                                      Die Anzahl der Einträge in der Liste sollte der Gesamtzahl der Räume entsprechen. \n",
    "                                      Mehrere Räume desselben Typs sollten jeweils einzeln aufgeführt werden.\"\"\")\n",
    "    balkon: bool = Field(description=\"Gibt an, ob das Objekt einen Balkon hat.\")\n",
    "    garten: bool = Field(description=\"Hat das Objekt einen Garten.\")\n",
    "    # NOTE: Werde ich nicht in meinen predictions berücksichtigen und könnte eventuell die Extraktion beeinflussen\n",
    "    #vermietet: bool = Field(description=\"Gibt an, ob das Objekt vermietet ist.\")\n",
    "    #miete: Optional[str] = Field(description=\"Monatliche Mieteinnahmen für das Objekt, falls vermietet.\")\n",
    "\n",
    "class Forclosure(BaseModel):\n",
    "    objekte: List[ForclosureObject] = Field(description=\"Liste der Zwangsversteigerungsobjekte, die zu diesem Fall gehören.\")\n",
    "    gesamtverkehrswert: int = Field(description=\"Gesamtverkehrswert aller Zwangsversteigerungsobjekte.\")\n",
    "    #beschreibung: str = Field(description=\"Allgemeine Beschreibung des Zwangsversteigerungsfalls.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=downloaded_model_path, n_ctx=16384, n_threads=8, n_gpu_layers=-1, verbose=False, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def pdf_to_string(file_path):\n",
    "    pdf_document = pymupdf.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from llama_cpp import LogitsProcessorList\n",
    "from lmformatenforcer import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))\n",
    "\n",
    "def llamacpp_with_character_level_parser(llm: Llama, prompt: str, character_level_parser: Optional[CharacterLevelParser]) -> str:\n",
    "    logits_processors: Optional[LogitsProcessorList] = None\n",
    "    if character_level_parser:\n",
    "        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm, character_level_parser)])\n",
    "    \n",
    "    output = llm(prompt, logits_processor=logits_processors, max_tokens=1000)\n",
    "    text: str = output['choices'][0]['text']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single files\n",
    "# file_path = \"../test_pdfs/359816.pdf\"\n",
    "# pdf_text = pdf_to_string(file_path)\n",
    "# question_with_schema = f'You MUST answer using the following json schema: {Forclosure.model_json_schema()}. \\n Please extract information about {pdf_text}.'\n",
    "# prompt = get_prompt(question_with_schema)\n",
    "# display_header(\"LLM Output with json schema enforcing:\")\n",
    "# result = llamacpp_with_character_level_parser(llm, prompt, JsonSchemaParser(Forclosure.model_json_schema()))\n",
    "# display_content(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 105869.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 113772.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 127734.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 15125.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 27138.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 359816.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 60782.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 61344.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 8033.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Output for 8786.pdf with JSON schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "test_pdfs_dir = \"../test_pdfs\"\n",
    "with open(\"./examples/5shot.txt\", \"r\") as shot:\n",
    "    examples = shot.read()\n",
    "\n",
    "results = []\n",
    "\n",
    "for file_name in os.listdir(test_pdfs_dir):\n",
    "    file_path = os.path.join(test_pdfs_dir, file_name)\n",
    "    \n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_text = pdf_to_string(file_path)\n",
    "        \n",
    "        question_with_schema = (\n",
    "            f'The following text lists examples for your task: {examples}.\\n'\n",
    "            f'You MUST answer using the following JSON schema: {Forclosure.model_json_schema()}.\\n'\n",
    "            f'Please extract information about the following PDF content: {pdf_text}.'\n",
    "        )\n",
    "        \n",
    "        prompt = get_prompt(question_with_schema)\n",
    "        display_header(f\"LLM Output for {file_name} with JSON schema enforcing:\")\n",
    "        \n",
    "        result = llamacpp_with_character_level_parser(\n",
    "            llm, prompt, JsonSchemaParser(Forclosure.model_json_schema())\n",
    "        )\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results written to output.txt\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"output.txt\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, item in enumerate(results):\n",
    "        file.write(f\"## Beispiel {i+1}\\n{item}\\n\")\n",
    "\n",
    "print(f\"Results written to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
